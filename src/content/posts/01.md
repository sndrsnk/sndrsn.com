---
title: Building a breath controller for live performance
description: A DIY breath controller for live improvisation performances created using Arduino, MaxMSP, and an AdaFruit UNTZtrument.
permalink: /posts/breath-controller-eis/
date: 2023-11-01
---

## Concept

My improvisation practice generally revolves around physical hardware, as I find the tactility and immediacy far more conductive to the creative flow than the often disruptive ritual of switching on and setting up my computer — a preference that developed as I was working in software engineering. With this in mind, I set out to develop a hybrid system to accompany my instrumental improvisations following my explorations into Pauline Oliveros's _The Expanded Instrument System_ (EIS) [[1]](#1).

My primary aim was to create the foundation for a dynamic composition that maintains a balance between spontaneity and control as the performance unfolds. I focused on stringed instruments given they require both of the performer’s hands to use, and in my own experience, offer the most opportunity for expression and tactility. 


My EIS aspects of a given improvised performance can be controlled through cycles of breath, presenting an opportunity for a reimagined textural accompaniment with which the performer can respond to.

 to which the performer can respond. Expanding on the idea of the performer’s response to these reprocessed sounds, I introduced elements of randomness to counterbalance their control and intention over the performance, encouraging opportunities for the performer to change their direction at any given moment.


I developed a MaxMSP patch for the audio processing I’m utilising spectral analysis to freeze segments of audio captured through a microphone and creating a drone by processing these segments through a set of dual multichannel loopers. Playback of these samples is slowed down and played back at different starting points using a multichannel mc.snowphasor object, with mc.updown being used to create attack and delay for smooth transitioning between samples. The advantage of capturing audio whilst the player is inhaling is that there’s no playback at this point and therefore the patch is not limited by latency introduced through real-time processing.

Note: I’ve implemented a breath control emulation patch that replays data captured via manual user input of a slider until the physical breath control module has been built. Its functionally the same with the start and end points of both the player’s inhalation and exhalation being utilised as triggers:


1. The player inhales and audio is frozen via the spectral processing patch.
2. Frozen audio is recorded to a buffer and processed through the first set of drone generators and played back.
3. Audio input is frozen a second time once recording of the first drone’s buffer is complete, which is subsequently recorded into the second drone’s buffer.
4. Once the end of the inhalation has been reached (denoted by a fixed point) and exhalation begins, both drones become audible and continue to play until the beginning of the next inhalation.

My artistic objective through these techniques is to introduce breathing as an additional aspect of instrumentation for the player. The act of inhaling allows the player to capture audio and control the playback of its resulting drones through exhalation as an accompaniment to improvised performance.


## References
<a id="1">[1]</a> 
[Oliveros, 2007](http://cycling74-web-uploads.s3.amazonaws.com/654eb437deb212469f7c3e6f/2023-11-29T11:27:19Z/The_Expanded_Instrument_System_Recent_De.pdf)